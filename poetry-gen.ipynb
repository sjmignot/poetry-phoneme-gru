{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# relative imports\n",
    "THIS_DIR = os.getcwd()\n",
    "sys.path.append(f'{THIS_DIR}/g2p_en')\n",
    "from g2p import G2p\n",
    "import expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# enables use of tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE SIMPLE RAW TEXT FILE FROM POETRY FILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets poems subfolder to merge all poems into large text file\n",
    "POEMS_FOLDER = 'poems/'\n",
    "POEM_FULL_PATH = os.path.join(THIS_DIR, POEMS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_files = [poem_file for poem_file in os.listdir(POEM_FULL_PATH)]\n",
    "all_poems_text = \"\"\n",
    "for poem_file in poem_files:\n",
    "    with open(os.path.join(POEM_FULL_PATH, poem_file), 'r') as f:\n",
    "        all_poems_text += f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALIZE AND CLEAN DATA\n",
    "\n",
    "1. Remove rare characters (those that appear less than 5 times),\n",
    "2. Substitute angled quotes with regular quotes,\n",
    "3. Convert character data into phonemes (I hypothesize that phoneme data will better represent poetic language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, rare_chars):\n",
    "    '''Helper function that removes angled quotes and rare characters'''\n",
    "    text = re.sub(r\"“\", '\"', text)\n",
    "    text = re.sub(r\"”\", '\"', text) \n",
    "    text = re.sub(r\"‘\", \"'\", text)\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(re.compile(\"|\".join(rare_chars)), \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Letter 7\n",
      "## Michael Palmer\n",
      "But the buried walls and our mouths of fragments,\n",
      " _no us but the snow staring at us_ . . .\n",
      "\n",
      "And you Mr. Ground-of_what, Mr. Text, Mr. Is-Was,\n",
      "can you calculate the ratio between wire and window,\n",
      "\n",
      "between tone and row, copula and carnival\n",
      "and can you reassemble light from the future-past\n",
      "\n",
      "in its parabolic nest\n",
      "or recite an entire winter's words,\n",
      "\n",
      "its liberties and psuedo-elegies,\n",
      "the shell of a street-car in mid-turn\n",
      "\n",
      "or scattered fires in the great hall\n",
      "I would say not-I here I'd say _The Book of Knots_\n",
      "\n",
      "I'd say undertows and currents and waterspouts,\n",
      "streaks of phosphorus and rivervine winds\n",
      "\n",
      "Dear Z, I'd say it's time, it's nearly time, it's almost, it's\n",
      "       just about, it's long\n",
      "past time now time now for the vex- for the vox- for the\n",
      "       voices of shadows,\n",
      "\n",
      "time for the prism letters, trinkets and shrouds,\n",
      "for a whirl in gauzy scarves around the wrecked piazza\n",
      "\n",
      "Messieurs-Dames, Meine Herren und Damen, our word-ballon,\n",
      "       you will note, is slow\n",
      "['#', ' ', 'L', 'EH1', 'T', 'ER0', ' ', 'S', 'EH1', 'V', 'AH0', 'N', ' ', '#', ' ', '#', ' ', 'M', 'AY1', 'K', 'AH0', 'L', ' ', 'P', 'AA1', 'M', 'ER0', ' ', 'B', 'AH1', 'T', ' ', 'DH', 'AH0', ' ', 'B', 'EH1', 'R', 'IY0', 'D', ' ', 'W', 'AO1', 'L', 'Z', ' ', 'AH0', 'N', 'D', ' ', 'AW1', 'ER0', ' ', 'M', 'AW1', 'TH', 'S', ' ', 'AH1', 'V', ' ', 'F', 'R', 'AE1', 'G', 'M', 'AH0', 'N', 'T', 'S', ' ', ',', ' ', 'N', 'OW1', ' ', 'AH1', 'S', ' ', 'B', 'AH1', 'T', ' ', 'DH', 'AH0', ' ', 'S', 'N', 'OW1', ' ', 'S', 'T', 'EH1', 'R', 'IH0', 'NG', ' ', 'AE1', 'T', ' ', 'AH1', 'S', ' ', '. . .', ' ', 'AH0', 'N', 'D', ' ', 'Y', 'UW1', ' ', 'M', 'IH1', 'S', 'T', 'ER0', ' ', '.', ' ', 'G', 'R', 'OW1', 'N', 'W', 'AA2', 'SH', 'T', 'AA2', 'F', ' ', ',', ' ', 'M', 'IH1', 'S', 'T', 'ER0', ' ', '.', ' ', 'T', 'EH1', 'K', 'S', 'T', ' ', ',', ' ', 'M', 'IH1', 'S', 'T', 'ER0', ' ', '.', ' ', 'IH1', 'W', 'AH0', 'S', ' ', ',', ' ', 'K', 'AE1', 'N', ' ', 'Y', 'UW1', ' ', 'K', 'AE1', 'L', 'K', 'Y', 'AH0', 'L', 'EY2', 'T', ' ', 'DH', 'AH0', ' ', 'R', 'EY1', 'SH', 'IY0', 'OW2', ' ', 'B', 'IH0', 'T', 'W', 'IY1', 'N', ' ', 'W', 'AY1', 'ER0', ' ', 'AH0', 'N', 'D', ' ', 'W', 'IH1', 'N', 'D', 'OW0', ' ', ',', ' ', 'B', 'IH0', 'T', 'W', 'IY1', 'N', ' ', 'T', 'OW1', 'N', ' ', 'AH0', 'N', 'D', ' ', 'R', 'OW1', ' ', ',', ' ', 'K', 'AA1', 'P', 'Y', 'AH0', 'L', 'AH0', ' ', 'AH0', 'N', 'D', ' ', 'K', 'AA1', 'R', 'N', 'AH0', 'V', 'AH0', 'L', ' ', 'AH0', 'N', 'D', ' ', 'K', 'AE1', 'N', ' ', 'Y', 'UW1', ' ', 'R', 'IY2', 'AH0', 'S', 'EH1', 'M', 'B', 'AH0', 'L', ' ', 'L', 'AY1', 'T', ' ', 'F', 'R', 'AH1', 'M', ' ', 'DH', 'AH0', ' ', 'F', 'Y', 'UW1', 'T', 'ER0', 'V', 'AE2', 'S', 'T', ' ', 'IH0', 'N', ' ', 'IH1', 'T', 'S', ' ', 'P', 'EH2', 'R', 'AH0', 'B', 'AA1', 'L', 'IH0', 'K', ' ', 'N', 'EH1', 'S', 'T', ' ', 'AO1', 'R', ' ', 'R', 'AH0', 'S', 'AY1', 'T', ' ', 'AE1', 'N', ' ', 'IH0', 'N', 'T', 'AY1', 'ER0', ' ', 'W', 'IH1', 'N', 'T', 'ER0', 'Z', ' ', 'W', 'ER1', 'D', 'Z', ' ', ',', ' ', 'IH1', 'T', 'S', ' ', 'L', 'IH1', 'B', 'ER0', 'T', 'IY0', 'Z', ' ', 'AH0', 'N', 'D', ' ', 'S', 'UW1', 'D', 'AH0', 'L', 'OW2', 'IY0', 'Z', ' ', ',', ' ', 'DH', 'AH0', ' ', 'SH', 'EH1', 'L', ' ', 'AH1', 'V', ' ', 'AH0', ' ', 'S', 'T', 'R', 'IY1', 'T', 'K', 'AA2', 'R', ' ', 'IH0', 'N', ' ', 'M', 'IH1', 'T', 'D', 'ER2', 'N', ' ', 'AO1', 'R', ' ', 'S', 'K', 'AE1', 'T', 'ER0', 'D', ' ', 'F', 'AY1', 'ER0', 'Z', ' ', 'IH0', 'N', ' ', 'DH', 'AH0', ' ', 'G', 'R', 'EY1', 'T', ' ', 'HH', 'AO1', 'L', ' ', 'AY1', ' ', 'W', 'UH1', 'D', ' ', 'S', 'EY1', ' ', 'N', 'OW1', 'T', 'IY0', ' ', 'HH', 'IY1', 'R', ' ', 'AY1', 'D', ' ', 'S', 'EY1', ' ', 'DH', 'IY1', ' ', 'B', 'UH1', 'K', ' ', 'AH1', 'V', ' ', 'N', 'AA1', 'T', 'S', ' ', '_', ' ', 'AY1', 'D', ' ', 'S', 'EY1', ' ', 'AH1', 'N', 'D', 'ER0', 'T', 'OW2', 'Z', ' ', 'AH0', 'N', 'D', ' ', 'K', 'ER1', 'AH0', 'N', 'T', 'S', ' ', 'AH0', 'N', 'D', ' ', 'W', 'AO1', 'T', 'ER0', 'S', 'T', 'W', 'AH2', 'P', 'T', 'S', ' ', ',', ' ', 'S', 'T', 'R', 'IY1', 'K', 'S', ' ', 'AH1', 'V', ' ', 'F', 'AA1', 'S', 'F', 'ER0', 'AH0', 'S', ' ', 'AH0', 'N', 'D', ' ', 'R', 'IH1', 'V', 'ER0', 'AY2', 'N', ' ', 'W', 'IH1', 'N', 'D', 'Z', ' ', 'D', 'IH1', 'R', ' ', 'Z', 'IY1', ' ', ',', ' ', 'AY1', 'D', ' ', 'S', 'EY1', ' ', 'IH1', 'T', 'S', ' ', 'T', 'AY1', 'M', ' ', ',', ' ', 'IH1', 'T', 'S', ' ', 'N', 'IH1', 'R', 'L', 'IY0', ' ', 'T', 'AY1', 'M', ' ', ',', ' ', 'IH1', 'T', 'S', ' ', 'AO1', 'L', 'M', 'OW2', 'S', 'T', ' ', ',', ' ', 'IH1', 'T', 'S', ' ', 'JH', 'AH1', 'S', 'T', ' ', 'AH0', 'B', 'AW1', 'T', ' ', ',', ' ', 'IH1', 'T', 'S', ' ', 'L', 'AO1', 'NG', ' ', 'P', 'AE1', 'S', 'T', ' ', 'T', 'AY1', 'M', ' ', 'N', 'AW1', ' ', 'T', 'AY1', 'M', ' ', 'N', 'AW1', ' ', 'F', 'AO1', 'R', ' ', 'DH', 'AH0', ' ', 'V', 'EH1', 'K', 'S', ' ', '-', ' ', 'F', 'AO1', 'R', ' ', 'DH', 'AH0', ' ', 'V', 'AA1', 'K', 'S', ' ', '-', ' ', 'F', 'AO1', 'R', ' ', 'DH', 'AH0', ' ', 'V', 'OY1', 'S', 'AH0', 'Z', ' ', 'AH1', 'V', ' ', 'SH', 'AE1', 'D', 'OW2', 'Z', ' ', ',', ' ', 'T', 'AY1', 'M', ' ', 'F', 'AO1', 'R', ' ', 'DH', 'AH0', ' ', 'P', 'R', 'IH1', 'Z', 'AH0', 'M', ' ', 'L', 'EH1', 'T', 'ER0', 'Z', ' ', ',', ' ', 'T', 'R', 'IH1', 'NG', 'K', 'AH0', 'T', 'S', ' ', 'AH0', 'N', 'D', ' ', 'SH', 'R', 'AW1', 'D', 'Z', ' ', ',', ' ', 'F', 'AO1', 'R', ' ', 'AH0', ' ', 'W', 'ER1', 'L', ' ', 'IH0', 'N', ' ', 'G', 'AW1', 'Z', 'IY0', ' ', 'S', 'K', 'AA1', 'R', 'V', 'Z', ' ', 'ER0', 'AW1', 'N', 'D', ' ', 'DH', 'AH0', ' ', 'R', 'EH1', 'K', 'T', ' ', 'P', 'IY0', 'AE1', 'Z', 'AH0', ' ', 'M', 'EH1', 'S', 'IH0', 'Z', 'M', 'ER0', 'D', 'Z', ' ', ',', ' ', 'M', 'IY1', 'N', ' ', 'HH', 'EH1', 'R', 'AH0', 'N', ' ', 'AH1', 'N', 'D', ' ', 'D', 'EY1', 'M', 'AH0', 'N', ' ', ',', ' ', 'AW1', 'ER0', ' ', 'W', 'ER1', 'D', 'B', 'L', 'AA1', 'R', 'OW0', ' ', ',', ' ', 'Y', 'UW1', ' ', 'W', 'IH1', 'L', ' ', 'N', 'OW1', 'T', ' ', ',', ' ', 'IH1', 'Z', ' ', 'S', 'L', 'OW1']\n"
     ]
    }
   ],
   "source": [
    "BUILD_PHONEMES = True \n",
    "if BUILD_PHONEMES:\n",
    "    rare_chars = Counter(all_poems_text)\n",
    "    rare_chars = [k for k,v in rare_chars.items() if v<=5]\n",
    "    all_poems_text = clean_text(all_poems_text, rare_chars)\n",
    "    # Uses a customized version of g2p that maintains newlines and other important punctuation characters\n",
    "    g2p = G2p()\n",
    "    all_poems_phonemes = g2p(all_poems_text[:1000])\n",
    "    with open('all_poems_phonemes.pickle', 'wb') as f:\n",
    "        pickle.dump(all_poems_phonemes, f)\n",
    "else: \n",
    "    with open('all_poems_phonemes.pickle', 'rb') as f:\n",
    "        all_poems_phonemes = pickle.load(f)\n",
    "        \n",
    "print(all_poems_text[:1000])\n",
    "print(all_poems_phonemes[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE CHAR TO INT MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', \"'\", ',', '-', '.', '.\\n\\n\\n. . .', '.\\n\\n   ...', '.\\n\\n. . .', '.\\n\\n...', '.\\n   .\\n   .', '.\\n  .', '.\\n . . .', '.\\n.\\n\\n.\\n.', '.\\n. .', '.\\n...', '.\\n...\\n...\\n...', '. .', '. .\\n.', '. .\\n. .', '. . .', '. . .\\n\\n. . .', '. . .\\n   . . .', '. . .  . . .', '. . . .', '. . . . .', '. . . . . . . . . . . .', '. . ..', '..', '...', '...\\n\\n\\n...', '...\\n\\n...', '...\\n...', '...\\n...\\n...', '?', 'AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0', 'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH', 'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1', 'EY2', 'F', 'G', 'HH', 'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW0', 'OW1', 'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH0', 'UH1', 'UH2', 'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH', '_', '__', '___']\n",
      "109 unique characters\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'e'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-993fdeb91a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcharacter_index_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindex_character_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtext_as_int_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcharacter_index_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_poems_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Show how the first 13 characters from the text are mapped to integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'{repr(all_poems_text[:13])} -- mapped to int -- > {text_as_int_array[:13]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-993fdeb91a7b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcharacter_index_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindex_character_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtext_as_int_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcharacter_index_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_poems_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Show how the first 13 characters from the text are mapped to integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'{repr(all_poems_text[:13])} -- mapped to int -- > {text_as_int_array[:13]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'e'"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(all_poems_phonemes))\n",
    "print(vocab)\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "character_index_map = {c:i for i, c in enumerate(vocab)}\n",
    "index_character_map = np.array(vocab)\n",
    "text_as_int_array = np.array([character_index_map[c] for c in all_poems_text])\n",
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print (f'{repr(all_poems_text[:13])} -- mapped to int -- > {text_as_int_array[:13]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(all_poems_text)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int_array)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "# Number of lstm layers\n",
    "lstm_layers = 1\n",
    "\n",
    "# Number of dense layers\n",
    "dense_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Dropout\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, lstm_layers):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    for i in range(lstm_layers):\n",
    "        model.add(Bidirectional(LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform')))\n",
    "        model.add(Dropout(0.1))\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE, \n",
    "  lstm_layers=lstm_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 151) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 256)           38656     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 151)           154775    \n",
      "=================================================================\n",
      "Total params: 4,131,735\n",
      "Trainable params: 4,131,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 151)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       5.01997\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "log_dir=f\"logs/fit/cgru_batch{BATCH_SIZE}_{rnn_units}units_embed{embedding_dim}_{str(time.time()//1)}\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8c046679e813e72b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8c046679e813e72b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 172 steps\n",
      "Epoch 1/10\n",
      "172/172 [==============================] - 333s 2s/step - loss: 2.0720\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 336s 2s/step - loss: 1.8957\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 307s 2s/step - loss: 1.8127\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 327s 2s/step - loss: 1.7374\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 324s 2s/step - loss: 1.6894\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 320s 2s/step - loss: 1.6504\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 600s 3s/step - loss: 1.6441\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 331s 2s/step - loss: 1.6248\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 326s 2s/step - loss: 1.6027\n",
      "Epoch 10/10\n",
      " 26/172 [===>..........................] - ETA: 4:42 - loss: 1.5969WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1720 batches). You may need to use the repeat() function when building your dataset.\n",
      " 26/172 [===>..........................] - ETA: 4:45 - loss: 1.5969"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit\n",
    "history = model.fit(dataset, epochs=EPOCHS, steps_per_epoch=172, callbacks=[checkpoint_callback, tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "def generate_text(model, start_string):\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [character_index_map[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = .5\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(index_character_map[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"Shall I compare thee to a summers day\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
