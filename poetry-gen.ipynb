{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "    # relative imports\n",
    "    THIS_DIR = os.getcwd()\n",
    "    THIS_DIR = os.path.join(THIS_DIR, 'drive/My Drive/poetry_phoneme_lstm')\n",
    "    sys.path.append(f'{THIS_DIR}/g2p_en')\n",
    "    from g2p import G2p\n",
    "    import expand\n",
    "    print(tf.test.gpu_device_name())\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "    THIS_DIR = os.getcwd()\n",
    "    sys.path.append(f'{THIS_DIR}/g2p_en')\n",
    "    from g2p import G2p\n",
    "    import expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# enables use of tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE SIMPLE RAW TEXT FILE FROM POETRY FILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samuel.mignot/Desktop/hobbies/code/jupyter-notebooks/poetry_phoneme_lstm/poems/\n"
     ]
    }
   ],
   "source": [
    "# sets poems subfolder to merge all poems into large text file\n",
    "POEMS_FOLDER = 'poems/'\n",
    "POEM_FULL_PATH = os.path.join(THIS_DIR, POEMS_FOLDER)\n",
    "print(POEM_FULL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_files = [poem_file for poem_file in os.listdir(POEM_FULL_PATH)]\n",
    "all_poems = []\n",
    "all_poems_text = \"\"\n",
    "for poem_file in poem_files:\n",
    "    with open(os.path.join(POEM_FULL_PATH, poem_file), 'r') as f:\n",
    "        all_poems_text+=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALIZE AND CLEAN DATA\n",
    "\n",
    "1. Remove rare characters (those that appear less than 5 times),\n",
    "2. Substitute angled quotes with regular quotes,\n",
    "3. Convert character data into phonemes (I hypothesize that phoneme data will better represent poetic language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, rare_chars):\n",
    "    '''Helper function that removes angled quotes and rare characters'''\n",
    "    text = re.sub(r\"“\", '\"', text)\n",
    "    text = re.sub(r\"”\", '\"', text) \n",
    "    text = re.sub(r\"‘\", \"'\", text)\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(re.compile(\"|\".join(rare_chars)), \"\", text)\n",
    "    text = re.sub(r\"\\n\", \"~\", text)\n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phonetic embedding\n",
    "phonetic_embedding = True\n",
    "\n",
    "if phonetic_embedding:\n",
    "    # Uses a customized version of g2p that maintains newlines and other important punctuation characters\n",
    "    g2p = G2p()\n",
    "    all_poems_text= g2p(all_poems_text)\n",
    "    phoneme_word_dict = g2p.word_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE CHAR TO INT MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', \"'\", ',', '-', '.', '. .', '. . .', '. . .  . . .', '. . . .', '. . . . .', '. . . . . . . . . . . .', '. . ..', '..', '...', '?', 'AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0', 'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH', 'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1', 'EY2', 'F', 'G', 'HH', 'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW0', 'OW1', 'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH0', 'UH1', 'UH2', 'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH', '_', '__', '___', '~']\n",
      "91 unique characters\n",
      "['#', ' ', 'L', 'EH1', 'T', 'ER0', ' ', 'S', 'EH1', 'V', 'AH0', 'N', ' '] -- mapped to int -- > [ 3  0 60 41 74 43  0 72 41 82 24 62  0]\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(all_poems_text))\n",
    "print(vocab)\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "character_index_map = {c:i for i, c in enumerate(vocab)}\n",
    "index_character_map = np.array(vocab)\n",
    "text_as_int_array = np.array([character_index_map[c] for c in all_poems_text])\n",
    "\n",
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print (f'{repr(all_poems_text[:13])} -- mapped to int -- > {text_as_int_array[:13]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text_as_int_array)//(seq_length+1)\n",
    "\n",
    "# drop remainder\n",
    "text_as_int_array = text_as_int_array[:examples_per_epoch*(seq_length+1)]\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int_array)\n",
    "\n",
    "sequences = [np.array(text_as_int_array[i:i + seq_length + 1]) for i in range(0, len(text_as_int_array), seq_length+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# BUFFER_SIZE = 10000\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 512 \n",
    "\n",
    "# Number of gru layers\n",
    "num_gru_layers = 1\n",
    "gru_dropout = .1\n",
    "is_bidirectional = False\n",
    "\n",
    "# Number of dense layers\n",
    "num_dense_layers = 1\n",
    "dense_dropout = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([np.array([sequence[:-1] for sequence in batch]) for batch in batches])\n",
    "# print(X.shape)\n",
    "# print(X[0][0])\n",
    "# y = np.array([np.array([sequence[1:] for sequence in batch]) for batch in batches])\n",
    "# print(y.shape)\n",
    "# print(y[0][0])\n",
    "\n",
    "sequences = np.array(sequences[:-(len(sequences)%BATCH_SIZE)])\n",
    "\n",
    "X = np.array([sequence[:-1] for sequence in sequences])\n",
    "y = np.array([sequence[1:] for sequence in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GRU, Bidirectional, Dropout\n",
    "\n",
    "def conditional_bidirection(layer, is_birdirectional):\n",
    "    if(is_bidirectional):\n",
    "        return Bidirectional(layer)\n",
    "    else:\n",
    "        return layer\n",
    "    \n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, num_gru_layers):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    for i in range(num_gru_layers):\n",
    "        model.add(conditional_bidirection(GRU(rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'),\n",
    "                  is_bidirectional)\n",
    "                 )\n",
    "    if(gru_dropout>0):\n",
    "        model.add(Dropout(gru_dropout))\n",
    "    for i in range(num_dense_layers):\n",
    "        model.add(Dense(vocab_size))\n",
    "        if(dense_dropout>0):\n",
    "            model.add(Dropout(dense_dropout))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE, \n",
    "  num_gru_layers=num_gru_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  0 60 ... 74 41 71]\n",
      " [63  0 22 ... 73 55 66]\n",
      " [36 52 74 ... 25 61  0]\n",
      " ...\n",
      " [60  0 39 ...  3  0 72]\n",
      " [72 53 60 ... 74 24 38]\n",
      " [51 43  0 ...  0 49 28]]\n",
      "(64, 100, 91) # (batch_size, sequence_length, vocab\n"
     ]
    }
   ],
   "source": [
    "input_example_batch, target_example_batch = (X[:BATCH_SIZE], y[:BATCH_SIZE])\n",
    "print(input_example_batch)\n",
    "example_batch_predictions = model(input_example_batch)\n",
    "print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (64, None, 256)           23296     \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (64, None, 512)           1182720   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (64, None, 512)           0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (64, None, 91)            46683     \n",
      "=================================================================\n",
      "Total params: 1,252,699\n",
      "Trainable params: 1,252,699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 91)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.510248\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=5\n",
    "model_name = f\"{'phoentic_' if phonetic_embedding else ''}char_{'b' if is_bidirectional else ''}gru_{num_gru_layers}l_{BATCH_SIZE}b_{rnn_units}u_{embedding_dim}e_{gru_dropout}d_dense_{num_dense_layers}l_{dense_dropout}d_{EPOCHS}epochs_{str(time.time()//1)}\"\n",
    "log_dir=f\"logs/fit/{model_name}\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 18506), started 0:02:17 ago. (Use '!kill 18506' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5029887b7c4ef9d3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5029887b7c4ef9d3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89460 samples, validate on 1420 samples\n",
      "Epoch 1/5\n",
      "38656/89460 [===========>..................] - ETA: 8:53 - loss: 2.3222 - sparse_categorical_accuracy: 0.3980"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit\n",
    "\n",
    "steps = (examples_per_epoch*63/64)//BATCH_SIZE\n",
    "\n",
    "history = model.fit(X, y, validation_split=(1/64), batch_size=BATCH_SIZE, steps_per_epoch=steps, epochs=EPOCHS, callbacks=[checkpoint_callback, tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_model() got an unexpected keyword argument 'lstm_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-f017a2af849e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkey_or_closest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: build_model() got an unexpected keyword argument 'lstm_layers'"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1, num_gru_layers=num_gru_layers)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "def key_or_closest(word):\n",
    "    try:\n",
    "        return phoneme_word_dict[word]\n",
    "    except:\n",
    "        keys = set(phoneme_word_dict.keys())\n",
    "        while(word not in keys):\n",
    "            word=word[:-1]\n",
    "            if len(word) == 1:\n",
    "                word = '#'\n",
    "                break\n",
    "        return phoneme_word_dict[word]\n",
    "    \n",
    "def generate_text(model, start_string):\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [character_index_map[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(index_character_map[predicted_id])\n",
    "\n",
    "  ph_text = (str(start_string) + ''.join(text_generated))\n",
    "  print(ph_text)\n",
    "  return \" \".join([key_or_closest(word) for word in ph_text.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string='# '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
