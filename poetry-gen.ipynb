{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# relative imports\n",
    "THIS_DIR = os.getcwd()\n",
    "sys.path.append(f'{THIS_DIR}/g2p_en')\n",
    "from g2p import G2p\n",
    "import expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables use of tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE SIMPLE RAW TEXT FILE FROM POETRY FILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets poems subfolder to merge all poems into large text file\n",
    "POEMS_FOLDER = 'poems/'\n",
    "POEM_FULL_PATH = os.path.join(THIS_DIR, POEMS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_files = [poem_file for poem_file in os.listdir(POEM_FULL_PATH)]\n",
    "all_poems = []\n",
    "all_poems_text = \"\"\n",
    "for poem_file in poem_files:\n",
    "    with open(os.path.join(POEM_FULL_PATH, poem_file), 'r') as f:\n",
    "        all_poems.append(\"\\n\".join(f.readlines()[2:]))\n",
    "        all_poems_text+=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALIZE AND CLEAN DATA\n",
    "\n",
    "1. Remove rare characters (those that appear less than 5 times),\n",
    "2. Substitute angled quotes with regular quotes,\n",
    "3. Convert character data into phonemes (I hypothesize that phoneme data will better represent poetic language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, rare_chars):\n",
    "    '''Helper function that removes angled quotes and rare characters'''\n",
    "    text = re.sub(r\"“\", '\"', text)\n",
    "    text = re.sub(r\"”\", '\"', text) \n",
    "    text = re.sub(r\"‘\", \"'\", text)\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(re.compile(\"|\".join(rare_chars)), \"\", text)\n",
    "    text = re.sub(r\"\\n\", \"~\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD_PHONEMES = False \n",
    "if BUILD_PHONEMES:\n",
    "    rare_chars = Counter(all_poems_text)\n",
    "    rare_chars = [k for k,v in rare_chars.items() if v<=5]\n",
    "    all_poems_text = clean_text(all_poems_text, rare_chars)\n",
    "    # Uses a customized version of g2p that maintains newlines and other important punctuation characters\n",
    "    g2p = G2p()\n",
    "    all_poems_phonemes = g2p(all_poems_text)\n",
    "    with open('all_poems_phonemes.pickle', 'wb') as f:\n",
    "        pickle.dump(all_poems_phonemes, f)\n",
    "    with open('phoneme_word_dict.pickle', 'wb') as f:\n",
    "        pickle.dump(g2p.word_map, f)\n",
    "else: \n",
    "    with open('all_poems_phonemes.pickle', 'rb') as f:\n",
    "        all_poems_phonemes = pickle.load(f)\n",
    "    with open('phoneme_word_dict.pickle', 'rb') as f:\n",
    "        phoneme_word_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE CHAR TO INT MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', \"'\", ',', '-', '.', '. .', '. . .', '. . .  . . .', '. . . .', '. . . . .', '. . . . . . . . . . . .', '. . ..', '..', '...', '?', 'AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0', 'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH', 'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1', 'EY2', 'F', 'G', 'HH', 'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW0', 'OW1', 'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH0', 'UH1', 'UH2', 'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH', '_', '__', '___', '~']\n",
      "91 unique characters\n",
      "['#', ' ', 'L', 'EH1', 'T', 'ER0', ' ', 'S', 'EH1', 'V', 'AH0', 'N', ' '] -- mapped to int -- > [ 3  0 60 41 74 43  0 72 41 82 24 62  0]\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(all_poems_phonemes))\n",
    "print(vocab)\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "character_index_map = {c:i for i, c in enumerate(vocab)}\n",
    "index_character_map = np.array(vocab)\n",
    "text_as_int_array = np.array([character_index_map[c] for c in all_poems_phonemes])\n",
    "\n",
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print (f'{repr(all_poems_phonemes[:13])} -- mapped to int -- > {text_as_int_array[:13]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(all_poems_phonemes)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int_array)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 512 \n",
    "\n",
    "# Number of lstm layers\n",
    "num_gru_layers = 1\n",
    "gru_dropout = .1\n",
    "\n",
    "is_bidirectional = True\n",
    "\n",
    "# Number of dense layers\n",
    "num_dense_layers = 1\n",
    "dense_dropout = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "num_elems = len(list(dataset.as_numpy_iterator()))\n",
    "val = .1\n",
    "val_dataset = dataset.take(int(num_elems*val))\n",
    "train_dataset = dataset.skip(int(num_elems*val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GRU, Bidirectional, Dropout\n",
    "\n",
    "def conditional_bidirection(layer, is_birdirectional):\n",
    "    if(is_bidirectional):\n",
    "        return Bidirectional(layer)\n",
    "    else:\n",
    "        return layer\n",
    "    \n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, num_gru_layers):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    for i in range(num_gru_layers):\n",
    "        model.add(conditional_bidirection(GRU(rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'),\n",
    "                  is_bidirectional)\n",
    "                 )\n",
    "    if(gru_dropout>0):\n",
    "        model.add(Dropout(gru_dropout))\n",
    "    for i in range(num_dense_layers):\n",
    "        model.add(Dense(vocab_size))\n",
    "        if(dense_dropout>0):\n",
    "            model.add(Dropout(dense_dropout))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE, \n",
    "  num_gru_layers=num_gru_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 91) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           23296     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (64, None, 1024)          2365440   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, None, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 91)            93275     \n",
      "=================================================================\n",
      "Total params: 2,482,011\n",
      "Trainable params: 2,482,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 91)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.512229\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "model_name = f\"{'b' if is_bidirectional else ''}gru_{num_gru_layers}l_{BATCH_SIZE}b_{rnn_units}u_{embedding_dim}e_{gru_dropout}d_dense_{num_dense_layers}l_{dense_dropout}d_{EPOCHS}epochs_{str(time.time()//1)}\"\n",
    "log_dir=f\"logs/fit/{model_name}\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9693), started 4:00:06 ago. (Use '!kill 9693' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7cfe0f8e40182403\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7cfe0f8e40182403\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 180 steps, validate for 142 steps\n",
      "Epoch 1/10\n",
      "180/180 [==============================] - 233s 1s/step - loss: 1.8373 - accuracy: 0.4179 - val_loss: 0.6986 - val_accuracy: 0.8426\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 228s 1s/step - loss: 1.0293 - accuracy: 0.7327 - val_loss: 1.1123 - val_accuracy: 0.8108\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 220s 1s/step - loss: 1.0236 - accuracy: 0.7701 - val_loss: 0.5935 - val_accuracy: 0.8670\n",
      "Epoch 4/10\n",
      " 12/180 [=>............................] - ETA: 2:46 - loss: 0.5824 - accuracy: 0.8826"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, steps_per_epoch=180, callbacks=[checkpoint_callback, tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1, lstm_layers=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "def key_or_closest(word):\n",
    "    try:\n",
    "        return phoneme_word_dict[word]\n",
    "    except:\n",
    "        keys = set(phoneme_word_dict.keys())\n",
    "        while(word not in keys):\n",
    "            word=word[:-1]\n",
    "            if len(word) == 1:\n",
    "                word = '#'\n",
    "                break\n",
    "        return phoneme_word_dict[word]\n",
    "    \n",
    "def generate_text(model, start_string):\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [character_index_map[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(index_character_map[predicted_id])\n",
    "\n",
    "  ph_text = (str(start_string) + ''.join(text_generated))\n",
    "  print(ph_text)\n",
    "  return \" \".join([key_or_closest(word) for word in ph_text.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string='# '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
