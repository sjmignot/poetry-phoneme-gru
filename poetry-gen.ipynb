{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "    # relative imports\n",
    "    THIS_DIR = os.getcwd()\n",
    "    THIS_DIR = os.path.join(THIS_DIR, 'drive/My Drive/poetry_phoneme_lstm')\n",
    "    sys.path.append(f'{THIS_DIR}/g2p_en')\n",
    "    from g2p import G2p\n",
    "    import expand\n",
    "    print(tf.test.gpu_device_name())\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "    THIS_DIR = os.getcwd()\n",
    "    sys.path.append(f'{THIS_DIR}/g2p_en')\n",
    "    from g2p import G2p\n",
    "    import expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables use of tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE SIMPLE RAW TEXT FILE FROM POETRY FILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samuelmignot/Desktop/hobbies/code/jupyter-notebooks/poetry_phoneme_lstm/poems/\n"
     ]
    }
   ],
   "source": [
    "# sets poems subfolder to merge all poems into large text file\n",
    "POEMS_FOLDER = 'poems/'\n",
    "POEM_FULL_PATH = os.path.join(THIS_DIR, POEMS_FOLDER)\n",
    "print(POEM_FULL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_files = [poem_file for poem_file in os.listdir(POEM_FULL_PATH)]\n",
    "all_poems = []\n",
    "all_poems_text = \"\"\n",
    "for poem_file in poem_files:\n",
    "    with open(os.path.join(POEM_FULL_PATH, poem_file), 'r') as f:\n",
    "        all_poems_text+=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALIZE AND CLEAN DATA\n",
    "\n",
    "1. Remove rare characters (those that appear less than 5 times),\n",
    "2. Substitute angled quotes with regular quotes,\n",
    "3. Convert character data into phonemes (I hypothesize that phoneme data will better represent poetic language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, rare_chars):\n",
    "    '''Helper function that removes angled quotes and rare characters'''\n",
    "    text = re.sub(r\"“\", '\"', text)\n",
    "    text = re.sub(r\"”\", '\"', text) \n",
    "    text = re.sub(r\"‘\", \"'\", text)\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(re.compile(\"|\".join(rare_chars)), \"\", text)\n",
    "    text = re.sub(r\"\\n\", \"~\", text)\n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phonetic embedding\n",
    "phonetic_embedding = True\n",
    "\n",
    "if phonetic_embedding:\n",
    "    # Uses a customized version of g2p that maintains newlines and other important punctuation characters\n",
    "    g2p = G2p()\n",
    "    all_poems_text= g2p(all_poems_text)\n",
    "    phoneme_word_dict = g2p.word_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE CHAR TO INT MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', \"'\", ',', '-', '.', '.   .   .', '.   ...', '.  .', '. .', '. . .', '. . .   . . .', '. . .  . . .', '. . . .', '. . . . .', '. . . . . . . . . . . .', '. . ..', '. . .. . .', '. ..', '. .. .', '..', '.. .', '.. . .', '...', '... ...', '?', 'AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0', 'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH', 'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1', 'EY2', 'F', 'G', 'HH', 'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW0', 'OW1', 'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH0', 'UH1', 'UH2', 'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH', '_', '__', '___', '~']\n",
      "101 unique characters\n",
      "['#', ' ', 'L', 'EH1', 'T', 'ER0', ' ', 'S', 'EH1', 'V', 'AH0', 'N', ' '] -- mapped to int -- > [ 3  0 70 51 84 53  0 82 51 92 34 72  0]\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(all_poems_text))\n",
    "print(vocab)\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "character_index_map = {c:i for i, c in enumerate(vocab)}\n",
    "index_character_map = np.array(vocab)\n",
    "text_as_int_array = np.array([character_index_map[c] for c in all_poems_text])\n",
    "\n",
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print (f'{repr(all_poems_text[:13])} -- mapped to int -- > {text_as_int_array[:13]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text_as_int_array)//(seq_length+1)\n",
    "\n",
    "# drop remainder\n",
    "text_as_int_array = text_as_int_array[:examples_per_epoch*(seq_length+1)]\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int_array)\n",
    "\n",
    "sequences = [np.array(text_as_int_array[i:i + seq_length + 1]) for i in range(0, len(text_as_int_array), seq_length+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# BUFFER_SIZE = 10000\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 512 \n",
    "\n",
    "# Number of gru layers\n",
    "num_gru_layers = 1\n",
    "gru_dropout = 0\n",
    "is_bidirectional = False\n",
    "\n",
    "# Number of dense layers\n",
    "num_dense_layers = 1\n",
    "dense_dropout = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81772\n",
      "81728\n"
     ]
    }
   ],
   "source": [
    "# X = np.array([np.array([sequence[:-1] for sequence in batch]) for batch in batches])\n",
    "# print(X.shape)\n",
    "# print(X[0][0])\n",
    "# y = np.array([np.array([sequence[1:] for sequence in batch]) for batch in batches])\n",
    "# print(y.shape)\n",
    "# print(y[0][0])\n",
    "print(len(sequences))\n",
    "sequences = np.array(sequences[:-(len(sequences)%BATCH_SIZE)])\n",
    "print(len(sequences))\n",
    "\n",
    "X = np.array([sequence[:-1] for sequence in sequences])\n",
    "y = np.array([sequence[1:] for sequence in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GRU, Bidirectional, Dropout\n",
    "\n",
    "def conditional_bidirection(layer, is_birdirectional):\n",
    "    if(is_bidirectional):\n",
    "        return Bidirectional(layer)\n",
    "    else:\n",
    "        return layer\n",
    "    \n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, num_gru_layers):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    for i in range(num_gru_layers):\n",
    "        model.add(conditional_bidirection(GRU(rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'),\n",
    "                  is_bidirectional)\n",
    "                 )\n",
    "    if(gru_dropout>0):\n",
    "        model.add(Dropout(gru_dropout))\n",
    "    for i in range(num_dense_layers):\n",
    "        model.add(Dense(vocab_size))\n",
    "        if(dense_dropout>0):\n",
    "            model.add(Dropout(dense_dropout))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE, \n",
    "  num_gru_layers=num_gru_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  0 70 ... 32 84  0]\n",
      " [82  0 12 ... 44 53  0]\n",
      " [72 48  0 ...  0 63 84]\n",
      " ...\n",
      " [49 57  0 ... 72 48  0]\n",
      " [63 69 70 ...  0 93 63]\n",
      " [ 0 71 36 ...  0 46 32]]\n",
      "(64, 100, 101) # (batch_size, sequence_length, vocab\n"
     ]
    }
   ],
   "source": [
    "input_example_batch, target_example_batch = (X[:BATCH_SIZE], y[:BATCH_SIZE])\n",
    "print(input_example_batch)\n",
    "example_batch_predictions = model(input_example_batch)\n",
    "print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           25856     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 512)           1182720   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 101)           51813     \n",
      "=================================================================\n",
      "Total params: 1,260,389\n",
      "Trainable params: 1,260,389\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 101)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.6149554\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "model_name = f\"{'phoentic_' if phonetic_embedding else ''}char_{'b' if is_bidirectional else ''}gru_{num_gru_layers}l_{BATCH_SIZE}b_{rnn_units}u_{embedding_dim}e_{gru_dropout}d_dense_{num_dense_layers}l_{dense_dropout}d_{EPOCHS}epochs_{str(time.time()//1)}\"\n",
    "log_dir=f\"logs/fit/{model_name}\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9693), started 1 day, 2:22:09 ago. (Use '!kill 9693' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-da56a4b9aa7a280f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-da56a4b9aa7a280f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79168 samples, validate on 2560 samples\n",
      "Epoch 1/10\n",
      "79168/79168 [==============================] - 678s 9ms/sample - loss: 2.2395 - sparse_categorical_accuracy: 0.3971 - val_loss: 2.0183 - val_sparse_categorical_accuracy: 0.4423\n",
      "Epoch 2/10\n",
      "79168/79168 [==============================] - 659s 8ms/sample - loss: 1.9436 - sparse_categorical_accuracy: 0.4643 - val_loss: 1.9375 - val_sparse_categorical_accuracy: 0.4626\n",
      "Epoch 3/10\n",
      "79168/79168 [==============================] - 656s 8ms/sample - loss: 1.8760 - sparse_categorical_accuracy: 0.4811 - val_loss: 1.9063 - val_sparse_categorical_accuracy: 0.4710\n",
      "Epoch 4/10\n",
      "79168/79168 [==============================] - 640s 8ms/sample - loss: 1.8407 - sparse_categorical_accuracy: 0.4900 - val_loss: 1.8889 - val_sparse_categorical_accuracy: 0.4754\n",
      "Epoch 5/10\n",
      "79168/79168 [==============================] - 669s 8ms/sample - loss: 1.8173 - sparse_categorical_accuracy: 0.4958 - val_loss: 1.8782 - val_sparse_categorical_accuracy: 0.4782\n",
      "Epoch 6/10\n",
      "79168/79168 [==============================] - 625s 8ms/sample - loss: 1.8004 - sparse_categorical_accuracy: 0.5001 - val_loss: 1.8749 - val_sparse_categorical_accuracy: 0.4785\n",
      "Epoch 7/10\n",
      "79168/79168 [==============================] - 625s 8ms/sample - loss: 1.7870 - sparse_categorical_accuracy: 0.5033 - val_loss: 1.8696 - val_sparse_categorical_accuracy: 0.4827\n",
      "Epoch 8/10\n",
      "79168/79168 [==============================] - 571s 7ms/sample - loss: 1.7763 - sparse_categorical_accuracy: 0.5061 - val_loss: 1.8692 - val_sparse_categorical_accuracy: 0.4821\n",
      "Epoch 9/10\n",
      "79168/79168 [==============================] - 565s 7ms/sample - loss: 1.7675 - sparse_categorical_accuracy: 0.5081 - val_loss: 1.8701 - val_sparse_categorical_accuracy: 0.4816\n",
      "Epoch 10/10\n",
      "79168/79168 [==============================] - 8126s 103ms/sample - loss: 1.7603 - sparse_categorical_accuracy: 0.5101 - val_loss: 1.8667 - val_sparse_categorical_accuracy: 0.4820\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit\n",
    "validation_sample = len(X)/10\n",
    "validation_sample -= validation_sample%64\n",
    "validation = 64*40/len(X)\n",
    "history = model.fit(X, y, validation_split=(64*40/len(X)), epochs=EPOCHS, callbacks=[checkpoint_callback, tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1, num_gru_layers=num_gru_layers)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "def key_or_closest(word):\n",
    "    try:\n",
    "        return phoneme_word_dict[word]\n",
    "    except:\n",
    "        keys = set(phoneme_word_dict.keys())\n",
    "        while(word not in keys):\n",
    "            word=word[:-1]\n",
    "            if len(word) == 1:\n",
    "                word = '#'\n",
    "                break\n",
    "        return phoneme_word_dict[word]\n",
    "    \n",
    "def generate_text(model, start_string):\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [character_index_map[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(index_character_map[predicted_id])\n",
    "\n",
    "  ph_text = (str(start_string) + ''.join(text_generated))\n",
    "  print(ph_text)\n",
    "  return \" \".join([key_or_closest(word) for word in ph_text.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string='# '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
